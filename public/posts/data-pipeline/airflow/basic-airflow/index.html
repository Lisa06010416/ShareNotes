<!doctype html><html lang=en dir=auto><head><script src="/ShareNotes/livereload.js?mindelay=10&amp;v=2&amp;port=52095&amp;path=ShareNotes/livereload" data-no-instant defer></script><meta charset=utf-8><meta http-equiv=X-UA-Compatible content="IE=edge"><meta name=viewport content="width=device-width,initial-scale=1,shrink-to-fit=no"><meta name=robots content="index, follow"><title>Apache Airflow Quick start: Installation, Demo DAG, Variables, XComs & Advanced @dag Settings | ML & DATA & CODE & CURIOSITY</title>
<meta name=keywords content="Airflow"><meta name=description content="My current work involves using Airflow to build data pipelines, so I set up a local Airflow environment to get a better grasp of the fundamentals. After all, at work we use a pre-configured environment.
I noticed that Airflow has many confusing time parameters, so I created a diagram to distinguish them.Here are more of my notes on Airflow, covering its fundamental operational flow, how to install and configure it, and ultimately how to successfully run a DAG. Check it out here:"><meta name=author content="Lisa Lin"><link rel=canonical href=http://localhost:52095/ShareNotes/posts/data-pipeline/airflow/basic-airflow/><meta name=google-site-verification content="0HsWvRWDrJupl4jAeKy9zVk0NTF6ttkgh2tUxuNON3Y"><meta name=msvalidate.01 content="XYZabc"><link crossorigin=anonymous href=/ShareNotes/assets/css/stylesheet.d6fcd20a4fb86efa4dfac8ec95da60244cc8871042183da1ef28e3a762ad79c8.css integrity="sha256-1vzSCk+4bvpN+sjsldpgJEzIhxBCGD2h7yjjp2Ktecg=" rel="preload stylesheet" as=style><link rel=icon href=http://localhost:52095/ShareNotes/images/favicon/curiousity_head.png><link rel=icon type=image/png sizes=16x16 href=http://localhost:52095/ShareNotes/images/favicon/curiousity_head.png><link rel=icon type=image/png sizes=32x32 href=http://localhost:52095/ShareNotes/images/favicon/curiousity_head.png><link rel=apple-touch-icon href=http://localhost:52095/ShareNotes/images/favicon/curiousity_head.png><link rel=mask-icon href=http://localhost:52095/ShareNotes/images/favicon/curiousity_head.png><meta name=theme-color content="#2e2e33"><meta name=msapplication-TileColor content="#2e2e33"><link rel=alternate hreflang=en href=http://localhost:52095/ShareNotes/posts/data-pipeline/airflow/basic-airflow/><noscript><style>#theme-toggle,.top-link{display:none}</style><style>@media(prefers-color-scheme:dark){:root{--theme:rgb(29, 30, 32);--entry:rgb(46, 46, 51);--primary:rgb(218, 218, 219);--secondary:rgb(155, 156, 157);--tertiary:rgb(65, 66, 68);--content:rgb(196, 196, 197);--code-block-bg:rgb(46, 46, 51);--code-bg:rgb(55, 56, 62);--border:rgb(51, 51, 51)}.list{background:var(--theme)}.list:not(.dark)::-webkit-scrollbar-track{background:0 0}.list:not(.dark)::-webkit-scrollbar-thumb{border-color:var(--theme)}}</style></noscript><meta property="og:url" content="http://localhost:52095/ShareNotes/posts/data-pipeline/airflow/basic-airflow/"><meta property="og:site_name" content="ML & DATA & CODE & CURIOSITY"><meta property="og:title" content="Apache Airflow Quick start: Installation, Demo DAG, Variables, XComs & Advanced @dag Settings"><meta property="og:description" content="My current work involves using Airflow to build data pipelines, so I set up a local Airflow environment to get a better grasp of the fundamentals. After all, at work we use a pre-configured environment.
I noticed that Airflow has many confusing time parameters, so I created a diagram to distinguish them.Here are more of my notes on Airflow, covering its fundamental operational flow, how to install and configure it, and ultimately how to successfully run a DAG. Check it out here:"><meta property="og:locale" content="en-us"><meta property="og:type" content="article"><meta property="article:section" content="posts"><meta property="article:published_time" content="2025-04-05T00:00:00+00:00"><meta property="article:modified_time" content="2025-04-05T00:00:00+00:00"><meta property="article:tag" content="Airflow"><meta property="og:image" content="http://localhost:52095/ShareNotes/images/opengraph.png"><meta name=twitter:card content="summary_large_image"><meta name=twitter:image content="http://localhost:52095/ShareNotes/images/opengraph.png"><meta name=twitter:title content="Apache Airflow Quick start: Installation, Demo DAG, Variables, XComs & Advanced @dag Settings"><meta name=twitter:description content="My current work involves using Airflow to build data pipelines, so I set up a local Airflow environment to get a better grasp of the fundamentals. After all, at work we use a pre-configured environment.
I noticed that Airflow has many confusing time parameters, so I created a diagram to distinguish them.Here are more of my notes on Airflow, covering its fundamental operational flow, how to install and configure it, and ultimately how to successfully run a DAG. Check it out here:"><script type=application/ld+json>{"@context":"https://schema.org","@type":"BreadcrumbList","itemListElement":[{"@type":"ListItem","position":1,"name":"Posts","item":"http://localhost:52095/ShareNotes/posts/"},{"@type":"ListItem","position":2,"name":"Apache Airflow Quick start: Installation, Demo DAG, Variables, XComs \u0026 Advanced @dag Settings","item":"http://localhost:52095/ShareNotes/posts/data-pipeline/airflow/basic-airflow/"}]}</script><script type=application/ld+json>{"@context":"https://schema.org","@type":"BlogPosting","headline":"Apache Airflow Quick start: Installation, Demo DAG, Variables, XComs \u0026 Advanced @dag Settings","name":"Apache Airflow Quick start: Installation, Demo DAG, Variables, XComs \u0026 Advanced @dag Settings","description":"My current work involves using Airflow to build data pipelines, so I set up a local Airflow environment to get a better grasp of the fundamentals. After all, at work we use a pre-configured environment.\nI noticed that Airflow has many confusing time parameters, so I created a diagram to distinguish them.Here are more of my notes on Airflow, covering its fundamental operational flow, how to install and configure it, and ultimately how to successfully run a DAG. Check it out here:\n","keywords":["Airflow"],"articleBody":"My current work involves using Airflow to build data pipelines, so I set up a local Airflow environment to get a better grasp of the fundamentals. After all, at work we use a pre-configured environment.\nI noticed that Airflow has many confusing time parameters, so I created a diagram to distinguish them.Here are more of my notes on Airflow, covering its fundamental operational flow, how to install and configure it, and ultimately how to successfully run a DAG. Check it out here:\nhttps://lisa06010416.github.io/ShareNotes/posts/data-pipeline/airflow/basic-airflow/\nWhat is Ariflow Airflow is a Python-based workflow management tool that schedules, executes, and monitors a series of tasks (workflows), automating and reliably handling data processing or other batch operations.\nDAG (Directed Acyclic Graph):\nThe core structure of a workflow. You can think of a DAG as a graph that defines a series of tasks and their relationships (order, dependencies).\nTask:\nA node within a DAG that represents a specific action, such as running a Python script, calling an API, executing a shell command, or submitting a Spark job. The task has different statuses to indicate its lifecycle—for example queued, running, success, failed, or skipped.\nWeb UI:\nA user-friendly visual interface where you can monitor DAG executions, view error logs, rerun tasks, and more.\nMetadata Database ​\tThe database record below important things:\n​\tTask Status: Records where each task is in its lifecycle, whether it has succeeded or failed, and its start and end times.\n​\tTask Queue: The Scheduler posts “what needs to be done next,” Workers read the board to pick up tasks, run them, and then post the results back.\n​\tCore Parameters: Connection details, variables, permissions, and other configuration items are stored in one place for easy reference and updates.\nScheduler, Executor, and Worker cooperate to orchestrate task execution：\nScheduler:\nScans the DAG and determines which task instances are ready to run.\nExecutor\nPushes those runnable task instances to the appropriate execution backend (local process, Celery queue, Kubernetes API, etc.).\nWorker\nListen to the queue, completes the tasks, and reports the results back to the metadata database.\nAirflow Install Installation from PyPI. Use follow constrain on terminal to install Airflow.\nAIRFLOW_VERSION=2.7.3 PYTHON_VERSION=3.8 CONSTRAINT_URL=\"https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt\" pip install \"apache-airflow==${AIRFLOW_VERSION}\" --constraint \"${CONSTRAINT_URL}\" When install airflow we need to assign constrain and provider.\nconstrain\nIn Airflow installation, a constraint is a file that limits or pins the versions of packages to ensure compatibility. It helps avoid version conflicts and makes the installation more stable. Provider\nIn Airflow, a provider is a package that adds extra functionality for working with specific tools, services, or platforms.\napache-airflow-providers-google: Add support for Google Cloud (like BigQuery, GCS) apache-airflow-providers-postgres: Add support for PostgreSQL Init Airflow export AIRFLOW_HOME=~/airflow airflow db init After running the init script, you can see the directory structure as follows:\nPath Description airflow/ Main Airflow directory ├── airflow.db Default database (SQLite by default) ├── webserver_config.py Configuration file for the Airflow web UI ├── airflow.cfg Main configuration file for Airflow, covering scheduler, webserver, executor, etc. ├── logs/ Directory containing log files generated by Airflow ├── dags/ Create at following step. Airflow load DAGs from The default path ~/airflow/dags Launch Airflow Launch Airflow UI\n# create a user airflow users create \\ --username admin \\ --firstname admin\\ --lastname admin \\ --role Admin \\ --email admin@example.com \\ --password admin # Launcher Web UI airflow webserver --port 8080 Navigate to http://0.0.0.0:8080/ and log in using the credentials admin/admin to access the UI.\nLauncher scheduler\nOpen another terminal enter the command airflow schedulerto launch the scheduler\nRun a demo dag on airflow Airflow load DAGs from The default path ~/airflow/dags. Use the following script to add a new DAG.\nmkdir -p ~/airflow/dags cat \u003c ~/airflow/dags/hello_airflow.py from airflow import DAG from airflow.operators.python import PythonOperator from datetime import datetime def hello(): print(\"Hello Airflow!\") with DAG( dag_id=\"hello_airflow\", start_date=datetime(2024, 1, 1), schedule_interval=None, catchup=False, tags=[\"demo\"] ) as dag: task = PythonOperator( task_id=\"say_hello\", python_callable=hello ) EOF Navigate to http://0.0.0.0:8080/ and run your DAG. If you can’t see your DAG in the web UI, run airflow dags list-import-errors or check the DAG import‑error message directly in the web UI.\nAirflow Variable How Airflow save Variable Airflow variables are stored using a key-value format, and by default, they are saved in an SQLite database located at /Users/{username}/airflow/airflow.db.\nSet and Get Variable Set directly in the Web UI:\nAdmin \u003e Variables \u003e Click +\nSet using Terminal:\nairflow variables set my_key1 my_value airflow variables set -j my_key2 '{\"key_in_json\": \"value_in_json\"}' Set using Python\nSet from airflow.models import Variable Variable.set(key=\"key1\", value=\"value1\") Variable.set(key=\"key2\", value={\"key_in_json\": \"value_in_json\"}, serialize_json=True) Get from airflow.models import Variable var1 = Variable.get(\"key1\") # Value returned by key1 json_var2 = Variable.get( \"key2\", deserialize_json=True )[\"key_in_json\"] print(var1) print(json_var2) Get by Context\nThe context is also a dictionary (key-value) data structure It stores variables related to the running DAG and the Airflow environment When using context in a function, you can directly access Variables without having to import them with from airflow.models import Variable def print_context_func(**context): print(context) Pass parameter between task XComs https://ithelp.ithome.com.tw/articles/10326663\nXComs(cross-communications) allows tasks to pass data between each other, but it’s designed for small amounts of data. Large datasets should be avoided when using XComs for data transfer.\ndef push_by_xcom(ti): ti.xcom_push(key=\"var1\", value=\"Apple\") print(\"Pushed var1='Apple' to XCom\") def pull_by_xcom(ti): var1 = ti.xcom_pull(task_ids=\"push_by_xcom\", key=\"var1\") print(\"Pulled var1 from XCom:\", var1) Airflow @dag Setting from airflow.decorators import dag from datetime import datetime, timedelta @dag( dag_id=\"my_dag\", # Unique ID of the DAG start_date=datetime(2023, 1, 1), # Start execution time of the DAG schedule_interval=\"@daily\", # Execution schedule catchup=False, # Whether to backfill past runs default_args={ # Default parameters for tasks (e.g., owner, retries, etc.) \"owner\": \"airflow\", \"retries\": 1, \"retry_delay\": timedelta(minutes=5), }, tags=[\"example\"], # Optional: Used to group or label the DAG ) def my_dag_function(): ... dag_id dag_id is the unique name you specify when defining a DAG in your Python code. This name will also appear in the Airflow web UI.\nVarious Time Parameter in Airflow Airflow executes a DAG after the schedule_interval has passed since the start_date. This means Airflow only starts processing data that occurred between start_date and start_date + schedule_interval after the full interval has completed.\nAssuming the DAG runs once per day (schedule_interval = @daily) and our task is responsible for processing data from the previous day, then for a given run, the time-related variables are as follows:\nexecution_date: 2024-04-01 data_interval_start: 2024-04-01 00:00 data_interval_end: 2024-04-02 00:00 If this run succeeds, then:\nprev_data_interval_start_success = 2024-04-01 00:00 prev_data_interval_end_success = 2024-04-02 00:00 There are the descriptionption for time paramerters in Airflow:\nTime Parameter Description start_date The start time of the DAG or Task. Airflow begins triggering the DAG from this time based on the schedule_interval. end_date The end time of the DAG or Task. After this point, the DAG will no longer be scheduled or executed. schedule_interval / data_interval The scheduling interval of the DAG. It determines how often the DAG runs. Examples: @once, @daily, '0 6 * * *', timedelta(hours=2) data_interval_start Airflow 2.2+ The start of the data interval (inclusive) for the current task run. data_interval_end Airflow 2.2+ The end of the data interval (exclusive) for the current task run. execution_date / logical_date / ds execution_date: Used in Airflow 1.x, represents the logical date the DAG run is for (not the actual run time).\nlogical_date: New term in Airflow 2.x, replaces execution_date with clearer meaning. ts The actual timestamp when the DAG run is triggered (actual execution time). prev_data_interval_start_success Airflow 2.x The start of the data interval (inclusive) for the last successful DAG or Task run. Set automatically by the scheduler based on history. May differ from execution_date due to skipped or backfilled runs. Commonly used to determine where to resume processing. prev_data_interval_end_success The end of the data interval (exclusive) for the last successful DAG or Task run. prev_start_date_success The execution date (logical time) of the last successful DAG run. Note: This is the logical run time, not the actual time. Different from prev_data_interval_start_success. Catchup When a DAG is enabled and the start_date is set to a time in the past, Airflow will automatically create DAG runs for all intervals from the start_date up to the current time. This process is called Catchup.\ncatchup=True：catchup=True: All DAG runs from start_date up to today will be backfilled (i.e., executed).\ncatchup=False：catchup=False: DAG runs will only be triggered from the current time onward; past DAG runs will not be backfilled.\ndag = DAG( dag_id='example_dag', start_date=datetime(2024, 1, 1), schedule_interval='@daily', catchup=True # default True ) Time Zone Airflow’s default timezone is UTC (Coordinated Universal Time).\nQuestion: If I set a timezone in my DAG, what time will it actually run?\nAirflow will schedule execution based on the timezone you set, but internally the execution time will be converted to UTC.\nCategory Time Your configured time According to the Asia/Taipei timezone Time stored by Airflow Converted to UTC and stored in the database Actual trigger time Will accurately match your configured timezone (e.g., 8 AM Taipei) Time shown in UI Based on default_timezone setting (can be shown as Asia/Taipei) 1. Global Timezone Configuration You can set the global default timezone by modifying the airflow.cfg file or using environment variables.\nIn the airflow.cfg, find the [core] section and add or modify the following setting:\n[core] default_timezone = Asia/Taipei 2. Setting Timezone Per DAG (Recommended) You can specify the timezone for each DAG individually, which is the recommended approach. Airflow uses the pendulum package to handle timezones.\nimport pendulum from airflow import DAG from datetime import datetime local_tz = pendulum.timezone(\"Asia/Taipei\") with DAG( dag_id=\"my_dag\", start_date=datetime(2023, 1, 1, tzinfo=local_tz), schedule_interval=\"@daily\", catchup=False, ) as dag: ... Retries and retry_delay retries\nThis defines the maximum number of times a task will be retried. The default is 0 (i.e., no retries). If you set it to 3, then when the task fails, Airflow will attempt to rerun it up to 3 additional times. retry_delay\nThis defines how long to wait between retries.\nIt should be specified as a datetime.timedelta object.\nFor example, to wait 5 minutes:\n\"retry_delay\": timedelta(minutes=5) Final Demo This demo Airflow DAG writes and reads Variables, prints runtime context, and showcases XCom data exchange across five sequential Python tasks scheduled to run every minute. Place it in ~/airflow/dags/, then run the DAG and examine the output in the Logs.\n# final_demo.py from datetime import datetime, timedelta from airflow import DAG from airflow.operators.python import PythonOperator from airflow.models import Variable # ─── Python callables ────────────────────────────────────────────────────────── def set_var(): Variable.set(key=\"key1\", value=\"value1\") Variable.set(key=\"key2\", value={\"key_in_json\": \"value_in_json\"}, serialize_json=True) def get_var(): var1 = Variable.get(\"key1\") json_var2 = Variable.get(\"key2\", deserialize_json=True)[\"key_in_json\"] print(\"key1:\", var1) print(\"key2.key_in_json:\", json_var2) def show_all_params(**context): print(\"Full context keys:\", list(context.keys())) def push_by_xcom(ti): ti.xcom_push(key=\"var1\", value=\"Apple\") print(\"Pushed var1='Apple' to XCom\") def pull_by_xcom(ti): var1 = ti.xcom_pull(task_ids=\"push_by_xcom\", key=\"var1\") print(\"Pulled var1 from XCom:\", var1) # ─── DAG definition ──────────────────────────────────────────────────────────── with DAG( dag_id=\"final_demo\", start_date=datetime(2024, 1, 1), schedule_interval=timedelta(minutes=1), # runs every minute catchup=False, tags=[\"demo\"], ) as dag: set_var_task = PythonOperator( task_id=\"set_var\", python_callable=set_var, ) get_var_task = PythonOperator( task_id=\"get_var\", python_callable=get_var, ) show_all_params_task = PythonOperator( task_id=\"show_all_params\", python_callable=show_all_params, ) push_by_xcom_task = PythonOperator( task_id=\"push_by_xcom\", python_callable=push_by_xcom, ) pull_by_xcom_task = PythonOperator( task_id=\"pull_by_xcom\", python_callable=pull_by_xcom, ) # ── Dependencies ── ( set_var_task \u003e\u003e get_var_task \u003e\u003e show_all_params_task \u003e\u003e push_by_xcom_task \u003e\u003e pull_by_xcom_task ) ","wordCount":"1825","inLanguage":"en","image":"http://localhost:52095/ShareNotes/images/opengraph.png","datePublished":"2025-04-05T00:00:00Z","dateModified":"2025-04-05T00:00:00Z","author":{"@type":"Person","name":"Lisa Lin"},"mainEntityOfPage":{"@type":"WebPage","@id":"http://localhost:52095/ShareNotes/posts/data-pipeline/airflow/basic-airflow/"},"publisher":{"@type":"Organization","name":"ML \u0026 DATA \u0026 CODE \u0026 CURIOSITY","logo":{"@type":"ImageObject","url":"http://localhost:52095/ShareNotes/images/favicon/curiousity_head.png"}}}</script></head><body id=top><script>localStorage.getItem("pref-theme")==="dark"?document.body.classList.add("dark"):localStorage.getItem("pref-theme")==="light"?document.body.classList.remove("dark"):window.matchMedia("(prefers-color-scheme: dark)").matches&&document.body.classList.add("dark")</script><header class=header><nav class=nav><div class=logo><a href=http://localhost:52095/ShareNotes/ accesskey=h title="Home (Alt + H)"><img src=http://localhost:52095/apple-touch-icon.png alt aria-label=logo height=35>Home</a><div class=logo-switches><button id=theme-toggle accesskey=t title="(Alt + T)"><svg id="moon" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><path d="M21 12.79A9 9 0 1111.21 3 7 7 0 0021 12.79z"/></svg><svg id="sun" width="24" height="18" viewBox="0 0 24 24" fill="none" stroke="currentcolor" stroke-width="2" stroke-linecap="round" stroke-linejoin="round"><circle cx="12" cy="12" r="5"/><line x1="12" y1="1" x2="12" y2="3"/><line x1="12" y1="21" x2="12" y2="23"/><line x1="4.22" y1="4.22" x2="5.64" y2="5.64"/><line x1="18.36" y1="18.36" x2="19.78" y2="19.78"/><line x1="1" y1="12" x2="3" y2="12"/><line x1="21" y1="12" x2="23" y2="12"/><line x1="4.22" y1="19.78" x2="5.64" y2="18.36"/><line x1="18.36" y1="5.64" x2="19.78" y2="4.22"/></svg></button></div></div><ul id=menu><li><a href=http://localhost:52095/ShareNotes/categories/ title=categories><span>categories</span></a></li><li><a href=http://localhost:52095/ShareNotes/tags/ title=tags><span>tags</span></a></li></ul></nav></header><main class=main><article class=post-single><header class=post-header><div class=breadcrumbs><a href=http://localhost:52095/ShareNotes/>Home</a>&nbsp;»&nbsp;<a href=http://localhost:52095/ShareNotes/posts/>Posts</a></div><h1 class="post-title entry-hint-parent">Apache Airflow Quick start: Installation, Demo DAG, Variables, XComs & Advanced @dag Settings</h1><div class=post-meta><span title='2025-04-05 00:00:00 +0000 UTC'>April 5, 2025</span>&nbsp;·&nbsp;9 min&nbsp;·&nbsp;1825 words&nbsp;·&nbsp;Lisa Lin&nbsp;|&nbsp;<a href=https://github.com/Lisa06010416/HugoEngineer/tree/master/content/posts/data-pipeline/airflow/basic-airflow/index.md rel="noopener noreferrer" target=_blank>Suggest Changes</a></div></header><div class=toc><details><summary accesskey=c title="(Alt + C)"><span class=details>Table of Contents</span></summary><div class=inner><nav id=TableOfContents><ul><li><a href=#what-is-ariflow>What is Ariflow</a></li><li><a href=#airflow-install>Airflow Install</a><ul><li><a href=#init-airflow>Init Airflow</a></li><li><a href=#launch-airflow>Launch Airflow</a></li><li><a href=#run-a-demo-dag-on-airflow>Run a demo dag on airflow</a></li></ul></li><li><a href=#airflow-variable>Airflow Variable</a><ul><li><a href=#how-airflow-save-variable>How Airflow save Variable</a></li><li><a href=#set-and-get-variable>Set and Get Variable</a></li></ul></li><li><a href=#pass-parameter-between-task>Pass parameter between task</a><ul><li><a href=#xcoms>XComs</a></li></ul></li><li><a href=#airflow-dag-setting>Airflow @dag Setting</a><ul><li><a href=#dag_id>dag_id</a></li><li><a href=#various-time-parameter-in-airflow>Various Time Parameter in Airflow</a></li><li><a href=#catchup>Catchup</a></li><li><a href=#time-zone>Time Zone</a></li><li><a href=#retries-and-retry_delay>Retries and retry_delay</a></li></ul></li><li><a href=#final-demo>Final Demo</a></li></ul></nav></div></details></div><div class=post-content><p>My current work involves using Airflow to build data pipelines, so I set up a local Airflow environment to get a better grasp of the fundamentals. After all, at work we use a pre-configured environment.</p><p>I noticed that Airflow has many confusing time parameters, so I created a diagram to distinguish them.Here are more of my notes on Airflow, covering its fundamental operational flow, how to install and configure it, and ultimately how to successfully run a DAG. Check it out here:</p><p><a href=https://lisa06010416.github.io/ShareNotes/posts/data-pipeline/airflow/basic-airflow/>https://lisa06010416.github.io/ShareNotes/posts/data-pipeline/airflow/basic-airflow/</a></p><h2 id=what-is-ariflow>What is Ariflow<a hidden class=anchor aria-hidden=true href=#what-is-ariflow>#</a></h2><p>Airflow is a Python-based workflow management tool that schedules, executes, and monitors a series of tasks (workflows), automating and reliably handling data processing or other batch operations.</p><p><img alt="Basic Airflow architecture" loading=lazy src=https://airflow.apache.org/docs/apache-airflow/2.0.1/_images/arch-diag-basic.png></p><ul><li><p><strong>DAG (Directed Acyclic Graph)</strong>:<br>The core structure of a workflow. You can think of a DAG as a graph that defines a series of tasks and their relationships (order, dependencies).</p><ul><li><p><strong>Task</strong>:<br>A node within a DAG that represents a specific action, such as running a Python script, calling an API, executing a shell command, or submitting a Spark job. The task has different statuses to indicate its lifecycle—for example <strong>queued</strong>, <strong>running</strong>, <strong>success</strong>, <strong>failed</strong>, or <strong>skipped</strong>.</p><p><img alt="task life-cycle" loading=lazy src=https://airflow.apache.org/docs/apache-airflow/2.0.1/_images/task_stages.png></p></li></ul></li><li><p><strong>Web UI</strong>:<br>A user-friendly visual interface where you can monitor DAG executions, view error logs, rerun tasks, and more.</p></li></ul><ul><li><strong>Metadata Database</strong></li></ul><p>​ The database record below important things:</p><p>​ <strong>Task Status:</strong> Records where each task is in its lifecycle, whether it has succeeded or failed, and its start and end times.</p><p>​ <strong>Task Queue:</strong> The Scheduler posts “what needs to be done next,” Workers read the board to pick up tasks, run them, and then post the results back.</p><p>​ <strong>Core Parameters:</strong> Connection details, variables, permissions, and other configuration items are stored in one place for easy reference and updates.</p><p><strong>Scheduler, Executor, and Worker cooperate to orchestrate task execution：</strong></p><p><img alt="core concept task" loading=lazy src=https://airflow.apache.org/docs/apache-airflow/stable/_images/task_lifecycle_diagram.png></p><ul><li><p><strong>Scheduler</strong>:<br>Scans the DAG and determines which task instances are ready to run.</p></li><li><p><strong>Executor</strong></p><p>Pushes those runnable task instances to the appropriate execution backend (local process, Celery queue, Kubernetes API, etc.).</p></li><li><p><strong>Worker</strong></p><p>Listen to the queue, completes the tasks, and reports the results back to the metadata database.</p></li></ul><h2 id=airflow-install>Airflow Install<a hidden class=anchor aria-hidden=true href=#airflow-install>#</a></h2><p><a href=https://airflow.apache.org/docs/apache-airflow/stable/installation/installing-from-pypi.html>Installation from PyPI</a>. Use follow constrain on terminal to install Airflow.</p><pre tabindex=0><code>AIRFLOW_VERSION=2.7.3
PYTHON_VERSION=3.8
CONSTRAINT_URL=&#34;https://raw.githubusercontent.com/apache/airflow/constraints-${AIRFLOW_VERSION}/constraints-${PYTHON_VERSION}.txt&#34;

pip install &#34;apache-airflow==${AIRFLOW_VERSION}&#34; --constraint &#34;${CONSTRAINT_URL}&#34;
</code></pre><p>When install airflow we need to assign constrain and provider.</p><p><strong>constrain</strong></p><ul><li>In Airflow installation, a <strong>constraint</strong> is a file that <strong>limits or pins the versions</strong> of packages to ensure compatibility. It helps avoid version conflicts and makes the installation more stable.</li></ul><p><strong>Provider</strong></p><p>In Airflow, a <strong>provider</strong> is a package that adds <strong>extra functionality</strong> for working with specific tools, services, or platforms.</p><ul><li>apache-airflow-providers-google: Add support for Google Cloud (like BigQuery, GCS)</li><li>apache-airflow-providers-postgres: Add support for PostgreSQL</li></ul><h3 id=init-airflow>Init Airflow<a hidden class=anchor aria-hidden=true href=#init-airflow>#</a></h3><pre tabindex=0><code class=language-# data-lang=#>export AIRFLOW_HOME=~/airflow
airflow db init
</code></pre><p>After running the init script, you can see the directory structure as follows:</p><table><thead><tr><th>Path</th><th>Description</th></tr></thead><tbody><tr><td><code>airflow/</code></td><td>Main Airflow directory</td></tr><tr><td>├── <code>airflow.db</code></td><td>Default database (SQLite by default)</td></tr><tr><td>├── <code>webserver_config.py</code></td><td>Configuration file for the Airflow web UI</td></tr><tr><td>├── <code>airflow.cfg</code></td><td>Main configuration file for Airflow, covering scheduler, webserver, executor, etc.</td></tr><tr><td>├── <code>logs/</code></td><td>Directory containing log files generated by Airflow</td></tr><tr><td>├── <code>dags/</code></td><td>Create at following step. Airflow load DAGs from The default path <code>~/airflow/dags</code></td></tr></tbody></table><h3 id=launch-airflow>Launch Airflow<a hidden class=anchor aria-hidden=true href=#launch-airflow>#</a></h3><p><strong>Launch Airflow UI</strong></p><pre tabindex=0><code># create a user
airflow users create \
    --username admin \
    --firstname admin\
    --lastname admin \
    --role Admin \
    --email admin@example.com \
    --password admin

# Launcher Web UI
airflow webserver --port 8080
</code></pre><p>Navigate to http://0.0.0.0:8080/ and log in using the credentials admin/admin to access the UI.</p><p><strong>Launcher scheduler</strong></p><p>Open another terminal enter the command <code>airflow scheduler</code>to launch the scheduler</p><h3 id=run-a-demo-dag-on-airflow>Run a demo dag on airflow<a hidden class=anchor aria-hidden=true href=#run-a-demo-dag-on-airflow>#</a></h3><p>Airflow load DAGs from The default path <code>~/airflow/dags</code>. Use the following script to add a new DAG.</p><pre tabindex=0><code>mkdir -p ~/airflow/dags

cat &lt;&lt;EOF &gt; ~/airflow/dags/hello_airflow.py
from airflow import DAG
from airflow.operators.python import PythonOperator
from datetime import datetime

def hello():
    print(&#34;Hello Airflow!&#34;)

with DAG(
    dag_id=&#34;hello_airflow&#34;,
    start_date=datetime(2024, 1, 1),
    schedule_interval=None,
    catchup=False,
    tags=[&#34;demo&#34;]
) as dag:
    task = PythonOperator(
        task_id=&#34;say_hello&#34;,
        python_callable=hello
    )
EOF
</code></pre><p>Navigate to http://0.0.0.0:8080/ and run your DAG. If you <strong>can’t</strong> see your DAG in the web UI, run <code>airflow dags list-import-errors</code> or check the DAG import‑error message directly in the web UI.</p><p><img alt=run_basic_dag loading=lazy src=/ShareNotes/posts/data-pipeline/airflow/basic-airflow/images/run_basic_dag.png></p><h2 id=airflow-variable>Airflow Variable<a hidden class=anchor aria-hidden=true href=#airflow-variable>#</a></h2><h3 id=how-airflow-save-variable>How Airflow save Variable<a hidden class=anchor aria-hidden=true href=#how-airflow-save-variable>#</a></h3><p>Airflow variables are stored using a key-value format, and by default, they are saved in an SQLite database located at <code>/Users/{username}/airflow/airflow.db</code>.</p><h3 id=set-and-get-variable>Set and Get Variable<a hidden class=anchor aria-hidden=true href=#set-and-get-variable>#</a></h3><ol><li><p>Set directly in the Web UI:<br>Admin > Variables > Click <code>+</code></p></li><li><p>Set using Terminal:</p><pre tabindex=0><code>airflow variables set my_key1 my_value

airflow variables set -j my_key2 &#39;{&#34;key_in_json&#34;: &#34;value_in_json&#34;}&#39;
</code></pre></li><li><p>Set using Python</p><ul><li>Set</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>airflow.models</span> <span class=kn>import</span> <span class=n>Variable</span>
</span></span><span class=line><span class=cl><span class=n>Variable</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=n>key</span><span class=o>=</span><span class=s2>&#34;key1&#34;</span><span class=p>,</span> <span class=n>value</span><span class=o>=</span><span class=s2>&#34;value1&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=n>Variable</span><span class=o>.</span><span class=n>set</span><span class=p>(</span><span class=n>key</span><span class=o>=</span><span class=s2>&#34;key2&#34;</span><span class=p>,</span> <span class=n>value</span><span class=o>=</span><span class=p>{</span><span class=s2>&#34;key_in_json&#34;</span><span class=p>:</span> <span class=s2>&#34;value_in_json&#34;</span><span class=p>},</span> <span class=n>serialize_json</span><span class=o>=</span><span class=kc>True</span><span class=p>)</span>
</span></span></code></pre></div><ul><li>Get</li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>from</span> <span class=nn>airflow.models</span> <span class=kn>import</span> <span class=n>Variable</span>
</span></span><span class=line><span class=cl><span class=n>var1</span> <span class=o>=</span> <span class=n>Variable</span><span class=o>.</span><span class=n>get</span><span class=p>(</span><span class=s2>&#34;key1&#34;</span><span class=p>)</span>  <span class=c1># Value returned by key1</span>
</span></span><span class=line><span class=cl><span class=n>json_var2</span> <span class=o>=</span> <span class=n>Variable</span><span class=o>.</span><span class=n>get</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=s2>&#34;key2&#34;</span><span class=p>,</span> <span class=n>deserialize_json</span><span class=o>=</span><span class=kc>True</span>
</span></span><span class=line><span class=cl><span class=p>)[</span><span class=s2>&#34;key_in_json&#34;</span><span class=p>]</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>var1</span><span class=p>)</span>
</span></span><span class=line><span class=cl><span class=nb>print</span><span class=p>(</span><span class=n>json_var2</span><span class=p>)</span>
</span></span></code></pre></div></li><li><p>Get by <strong>Context</strong></p><ul><li>The context is also a dictionary (key-value) data structure</li><li>It stores variables related to the running DAG and the Airflow environment</li><li>When using <code>context</code> in a function, you can directly access Variables without having to import them with <code>from airflow.models import Variable</code></li></ul><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=k>def</span> <span class=nf>print_context_func</span><span class=p>(</span><span class=o>**</span><span class=n>context</span><span class=p>):</span>
</span></span><span class=line><span class=cl>    <span class=nb>print</span><span class=p>(</span><span class=n>context</span><span class=p>)</span>
</span></span></code></pre></div></li></ol><h2 id=pass-parameter-between-task>Pass parameter between task<a hidden class=anchor aria-hidden=true href=#pass-parameter-between-task>#</a></h2><h3 id=xcoms>XComs<a hidden class=anchor aria-hidden=true href=#xcoms>#</a></h3><p><a href=https://ithelp.ithome.com.tw/articles/10326663>https://ithelp.ithome.com.tw/articles/10326663</a></p><p>XComs(cross-communications) allows tasks to pass data between each other, but it&rsquo;s designed for small amounts of data. Large datasets should be avoided when using XComs for data transfer.</p><pre tabindex=0><code>def push_by_xcom(ti):
    ti.xcom_push(key=&#34;var1&#34;, value=&#34;Apple&#34;)
    print(&#34;Pushed var1=&#39;Apple&#39; to XCom&#34;)

def pull_by_xcom(ti):
    var1 = ti.xcom_pull(task_ids=&#34;push_by_xcom&#34;, key=&#34;var1&#34;)
    print(&#34;Pulled var1 from XCom:&#34;, var1)
</code></pre><h2 id=airflow-dag-setting>Airflow @dag Setting<a hidden class=anchor aria-hidden=true href=#airflow-dag-setting>#</a></h2><pre tabindex=0><code>from airflow.decorators import dag
from datetime import datetime, timedelta

@dag(
    dag_id=&#34;my_dag&#34;,  # Unique ID of the DAG
    start_date=datetime(2023, 1, 1),  # Start execution time of the DAG
    schedule_interval=&#34;@daily&#34;,  # Execution schedule
    catchup=False,  # Whether to backfill past runs
    default_args={  # Default parameters for tasks (e.g., owner, retries, etc.)
        &#34;owner&#34;: &#34;airflow&#34;,
        &#34;retries&#34;: 1,
        &#34;retry_delay&#34;: timedelta(minutes=5),
    },
    tags=[&#34;example&#34;],  # Optional: Used to group or label the DAG
)
def my_dag_function():
    ...
</code></pre><h3 id=dag_id>dag_id<a hidden class=anchor aria-hidden=true href=#dag_id>#</a></h3><p><code>dag_id</code> is the unique name you specify when defining a DAG in your Python code. This name will also appear in the Airflow web UI.</p><h3 id=various-time-parameter-in-airflow>Various Time Parameter in Airflow<a hidden class=anchor aria-hidden=true href=#various-time-parameter-in-airflow>#</a></h3><p>Airflow executes a DAG <strong>after</strong> the <code>schedule_interval</code> has passed since the <code>start_date</code>. This means Airflow only starts processing data that occurred <strong>between</strong> <code>start_date</code> and <code>start_date + schedule_interval</code> <strong>after</strong> the full interval has completed.</p><p><img alt=airflow_dag_time_parameter loading=lazy src=/ShareNotes/posts/data-pipeline/airflow/basic-airflow/images/airflow_dag_time_parameter.png></p><p>Assuming the DAG runs once per day (<code>schedule_interval = @daily</code>) and our task is responsible for processing data from the previous day, then for a given run, the time-related variables are as follows:</p><ul><li><code>execution_date</code>: 2024-04-01</li><li><code>data_interval_start</code>: 2024-04-01 00:00</li><li><code>data_interval_end</code>: 2024-04-02 00:00</li></ul><p>If this run succeeds, then:</p><ul><li><code>prev_data_interval_start_success</code> = 2024-04-01 00:00</li><li><code>prev_data_interval_end_success</code> = 2024-04-02 00:00</li></ul><p>There are the descriptionption for time paramerters in Airflow:</p><table><thead><tr><th>Time Parameter</th><th>Description</th></tr></thead><tbody><tr><td><code>start_date</code></td><td>The <strong>start time</strong> of the DAG or Task. Airflow begins triggering the DAG from this time based on the <code>schedule_interval</code>.</td></tr><tr><td><code>end_date</code></td><td>The <strong>end time</strong> of the DAG or Task. After this point, the DAG will no longer be scheduled or executed.</td></tr><tr><td><code>schedule_interval</code> / <code>data_interval</code></td><td>The scheduling interval of the DAG. It determines how often the DAG runs.<br>Examples: <code>@once</code>, <code>@daily</code>, <code>'0 6 * * *'</code>, <code>timedelta(hours=2)</code></td></tr><tr><td><code>data_interval_start</code></td><td><strong>Airflow 2.2+</strong><br>The <strong>start of the data interval (inclusive)</strong> for the current task run.</td></tr><tr><td><code>data_interval_end</code></td><td><strong>Airflow 2.2+</strong><br>The <strong>end of the data interval (exclusive)</strong> for the current task run.</td></tr><tr><td><code>execution_date</code> / <code>logical_date</code> / <code>ds</code></td><td><code>execution_date</code>: Used in <strong>Airflow 1.x</strong>, represents the <strong>logical date</strong> the DAG run is for (not the actual run time).<br><code>logical_date</code>: New term in <strong>Airflow 2.x</strong>, replaces <code>execution_date</code> with clearer meaning.</td></tr><tr><td><code>ts</code></td><td>The actual timestamp when the DAG run is triggered (actual execution time).</td></tr><tr><td><code>prev_data_interval_start_success</code></td><td><strong>Airflow 2.x</strong><br>The <strong>start of the data interval</strong> (inclusive) for the <strong>last successful</strong> DAG or Task run.<br>Set automatically by the scheduler based on history. May differ from <code>execution_date</code> due to skipped or backfilled runs. Commonly used to determine where to resume processing.</td></tr><tr><td><code>prev_data_interval_end_success</code></td><td>The <strong>end of the data interval</strong> (exclusive) for the <strong>last successful</strong> DAG or Task run.</td></tr><tr><td><code>prev_start_date_success</code></td><td>The <strong>execution date</strong> (logical time) of the <strong>last successful</strong> DAG run.<br>Note: This is the logical run time, not the actual time. Different from <code>prev_data_interval_start_success</code>.</td></tr></tbody></table><h3 id=catchup>Catchup<a hidden class=anchor aria-hidden=true href=#catchup>#</a></h3><p>When a DAG is enabled and the <code>start_date</code> is set to a time in the past, Airflow will automatically create DAG runs for all intervals from the <code>start_date</code> up to the current time. This process is called <strong>Catchup</strong>.</p><ul><li><p><code>catchup=True</code>：<strong><code>catchup=True</code>:</strong> All DAG runs from <code>start_date</code> up to today will be backfilled (i.e., executed).</p></li><li><p><code>catchup=False</code>：<strong><code>catchup=False</code>:</strong> DAG runs will only be triggered from the current time onward; past DAG runs will not be backfilled.</p></li></ul><pre tabindex=0><code>dag = DAG(
    dag_id=&#39;example_dag&#39;,
    start_date=datetime(2024, 1, 1),
    schedule_interval=&#39;@daily&#39;,
    catchup=True  # default True
)
</code></pre><h3 id=time-zone>Time Zone<a hidden class=anchor aria-hidden=true href=#time-zone>#</a></h3><p>Airflow&rsquo;s default timezone is <strong>UTC (Coordinated Universal Time)</strong>.</p><p><strong>Question: If I set a timezone in my DAG, what time will it actually run?</strong></p><p>Airflow will schedule execution based on the timezone you set, but internally the execution time will be converted to UTC.</p><table><thead><tr><th>Category</th><th>Time</th></tr></thead><tbody><tr><td>Your configured time</td><td>According to the Asia/Taipei timezone</td></tr><tr><td>Time stored by Airflow</td><td>Converted to UTC and stored in the database</td></tr><tr><td>Actual trigger time</td><td>Will accurately match your configured timezone (e.g., 8 AM Taipei)</td></tr><tr><td>Time shown in UI</td><td>Based on <code>default_timezone</code> setting (can be shown as Asia/Taipei)</td></tr></tbody></table><hr><h4 id=1-global-timezone-configuration>1. <strong>Global Timezone Configuration</strong><a hidden class=anchor aria-hidden=true href=#1-global-timezone-configuration>#</a></h4><p>You can set the global default timezone by modifying the <code>airflow.cfg</code> file or using environment variables.</p><p>In the <code>airflow.cfg</code>, find the <code>[core]</code> section and add or modify the following setting:</p><pre tabindex=0><code>[core]
default_timezone = Asia/Taipei
</code></pre><hr><h4 id=2-setting-timezone-per-dag-recommended>2. <strong>Setting Timezone Per DAG (Recommended)</strong><a hidden class=anchor aria-hidden=true href=#2-setting-timezone-per-dag-recommended>#</a></h4><p>You can specify the timezone for each DAG individually, which is the recommended approach. Airflow uses the <code>pendulum</code> package to handle timezones.</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=kn>import</span> <span class=nn>pendulum</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>airflow</span> <span class=kn>import</span> <span class=n>DAG</span>
</span></span><span class=line><span class=cl><span class=kn>from</span> <span class=nn>datetime</span> <span class=kn>import</span> <span class=n>datetime</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=n>local_tz</span> <span class=o>=</span> <span class=n>pendulum</span><span class=o>.</span><span class=n>timezone</span><span class=p>(</span><span class=s2>&#34;Asia/Taipei&#34;</span><span class=p>)</span>
</span></span><span class=line><span class=cl>
</span></span><span class=line><span class=cl><span class=k>with</span> <span class=n>DAG</span><span class=p>(</span>
</span></span><span class=line><span class=cl>    <span class=n>dag_id</span><span class=o>=</span><span class=s2>&#34;my_dag&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>start_date</span><span class=o>=</span><span class=n>datetime</span><span class=p>(</span><span class=mi>2023</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=mi>1</span><span class=p>,</span> <span class=n>tzinfo</span><span class=o>=</span><span class=n>local_tz</span><span class=p>),</span>
</span></span><span class=line><span class=cl>    <span class=n>schedule_interval</span><span class=o>=</span><span class=s2>&#34;@daily&#34;</span><span class=p>,</span>
</span></span><span class=line><span class=cl>    <span class=n>catchup</span><span class=o>=</span><span class=kc>False</span><span class=p>,</span>
</span></span><span class=line><span class=cl><span class=p>)</span> <span class=k>as</span> <span class=n>dag</span><span class=p>:</span>
</span></span><span class=line><span class=cl>    <span class=o>...</span>
</span></span></code></pre></div><h3 id=retries-and-retry_delay>Retries and retry_delay<a hidden class=anchor aria-hidden=true href=#retries-and-retry_delay>#</a></h3><p><strong>retries</strong></p><ul><li>This defines <strong>the maximum number of times a task will be retried</strong>.</li><li>The default is <code>0</code> (i.e., no retries).</li><li>If you set it to <code>3</code>, then when the task fails, Airflow will attempt to rerun it up to <strong>3 additional times</strong>.</li></ul><hr><p><strong>retry_delay</strong></p><ul><li><p>This defines <strong>how long to wait between retries</strong>.</p></li><li><p>It should be specified as a <code>datetime.timedelta</code> object.<br>For example, to wait 5 minutes:</p><div class=highlight><pre tabindex=0 class=chroma><code class=language-python data-lang=python><span class=line><span class=cl><span class=s2>&#34;retry_delay&#34;</span><span class=p>:</span> <span class=n>timedelta</span><span class=p>(</span><span class=n>minutes</span><span class=o>=</span><span class=mi>5</span><span class=p>)</span>
</span></span></code></pre></div></li></ul><h2 id=final-demo>Final Demo<a hidden class=anchor aria-hidden=true href=#final-demo>#</a></h2><p>This demo Airflow DAG writes and reads Variables, prints runtime context, and showcases XCom data exchange across five sequential Python tasks scheduled to run every minute. Place it in <code>~/airflow/dags/</code>, then run the DAG and examine the output in the Logs.</p><pre tabindex=0><code># final_demo.py
from datetime import datetime, timedelta
from airflow import DAG
from airflow.operators.python import PythonOperator
from airflow.models import Variable


# ─── Python callables ──────────────────────────────────────────────────────────
def set_var():
    Variable.set(key=&#34;key1&#34;, value=&#34;value1&#34;)
    Variable.set(key=&#34;key2&#34;,
                 value={&#34;key_in_json&#34;: &#34;value_in_json&#34;},
                 serialize_json=True)


def get_var():
    var1 = Variable.get(&#34;key1&#34;)
    json_var2 = Variable.get(&#34;key2&#34;, deserialize_json=True)[&#34;key_in_json&#34;]
    print(&#34;key1:&#34;, var1)
    print(&#34;key2.key_in_json:&#34;, json_var2)


def show_all_params(**context):
    print(&#34;Full context keys:&#34;, list(context.keys()))


def push_by_xcom(ti):
    ti.xcom_push(key=&#34;var1&#34;, value=&#34;Apple&#34;)
    print(&#34;Pushed var1=&#39;Apple&#39; to XCom&#34;)


def pull_by_xcom(ti):
    var1 = ti.xcom_pull(task_ids=&#34;push_by_xcom&#34;, key=&#34;var1&#34;)
    print(&#34;Pulled var1 from XCom:&#34;, var1)


# ─── DAG definition ────────────────────────────────────────────────────────────
with DAG(
    dag_id=&#34;final_demo&#34;,
    start_date=datetime(2024, 1, 1),
    schedule_interval=timedelta(minutes=1),   # runs every minute
    catchup=False,
    tags=[&#34;demo&#34;],
) as dag:

    set_var_task = PythonOperator(
        task_id=&#34;set_var&#34;,
        python_callable=set_var,
    )

    get_var_task = PythonOperator(
        task_id=&#34;get_var&#34;,
        python_callable=get_var,
    )

    show_all_params_task = PythonOperator(
        task_id=&#34;show_all_params&#34;,
        python_callable=show_all_params,
    )

    push_by_xcom_task = PythonOperator(
        task_id=&#34;push_by_xcom&#34;,
        python_callable=push_by_xcom,
    )

    pull_by_xcom_task = PythonOperator(
        task_id=&#34;pull_by_xcom&#34;,
        python_callable=pull_by_xcom,
    )

    # ── Dependencies ──
    (
        set_var_task
        &gt;&gt; get_var_task
        &gt;&gt; show_all_params_task
        &gt;&gt; push_by_xcom_task
        &gt;&gt; pull_by_xcom_task
    )
</code></pre></div><footer class=post-footer><ul class=post-tags><li><a href=http://localhost:52095/ShareNotes/tags/airflow/>Airflow</a></li></ul><nav class=paginav><a class=next href=http://localhost:52095/ShareNotes/posts/others/using-hugo-and-github-pages-step-by-step-guide/><span class=title>Next »</span><br><span>A step-by-step guide to using Hugo and GitHub Pages for personal website creation</span></a></nav><ul class=share-buttons><li><a target=_blank rel="noopener noreferrer" aria-label="share Apache Airflow Quick start: Installation, Demo DAG, Variables, XComs & Advanced @dag Settings on x" href="https://x.com/intent/tweet/?text=Apache%e2%80%afAirflow%20Quick%20start%3a%20Installation%2c%20Demo%20DAG%2c%20Variables%2c%20XComs%20%26%20Advanced%e2%80%af%40dag%20Settings&amp;url=http%3a%2f%2flocalhost%3a52095%2fShareNotes%2fposts%2fdata-pipeline%2fairflow%2fbasic-airflow%2f&amp;hashtags=Airflow"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446C483.971.0 512 28.03 512 62.554zM269.951 190.75 182.567 75.216H56L207.216 272.95 63.9 436.783h61.366L235.9 310.383l96.667 126.4H456L298.367 228.367l134-153.151H371.033zM127.633 110h36.468l219.38 290.065H349.5z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Apache Airflow Quick start: Installation, Demo DAG, Variables, XComs & Advanced @dag Settings on linkedin" href="https://www.linkedin.com/shareArticle?mini=true&amp;url=http%3a%2f%2flocalhost%3a52095%2fShareNotes%2fposts%2fdata-pipeline%2fairflow%2fbasic-airflow%2f&amp;title=Apache%e2%80%afAirflow%20Quick%20start%3a%20Installation%2c%20Demo%20DAG%2c%20Variables%2c%20XComs%20%26%20Advanced%e2%80%af%40dag%20Settings&amp;summary=Apache%e2%80%afAirflow%20Quick%20start%3a%20Installation%2c%20Demo%20DAG%2c%20Variables%2c%20XComs%20%26%20Advanced%e2%80%af%40dag%20Settings&amp;source=http%3a%2f%2flocalhost%3a52095%2fShareNotes%2fposts%2fdata-pipeline%2fairflow%2fbasic-airflow%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM160.461 423.278V197.561h-75.04v225.717h75.04zm270.539.0V293.839c0-69.333-37.018-101.586-86.381-101.586-39.804.0-57.634 21.891-67.617 37.266v-31.958h-75.021c.995 21.181.0 225.717.0 225.717h75.02V297.222c0-6.748.486-13.492 2.474-18.315 5.414-13.475 17.767-27.434 38.494-27.434 27.135.0 38.007 20.707 38.007 51.037v120.768H431zM123.448 88.722C97.774 88.722 81 105.601 81 127.724c0 21.658 16.264 39.002 41.455 39.002h.484c26.165.0 42.452-17.344 42.452-39.002-.485-22.092-16.241-38.954-41.943-39.002z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Apache Airflow Quick start: Installation, Demo DAG, Variables, XComs & Advanced @dag Settings on reddit" href="https://reddit.com/submit?url=http%3a%2f%2flocalhost%3a52095%2fShareNotes%2fposts%2fdata-pipeline%2fairflow%2fbasic-airflow%2f&title=Apache%e2%80%afAirflow%20Quick%20start%3a%20Installation%2c%20Demo%20DAG%2c%20Variables%2c%20XComs%20%26%20Advanced%e2%80%af%40dag%20Settings"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zM446 265.638c0-22.964-18.616-41.58-41.58-41.58-11.211.0-21.361 4.457-28.841 11.666-28.424-20.508-67.586-33.757-111.204-35.278l18.941-89.121 61.884 13.157c.756 15.734 13.642 28.29 29.56 28.29 16.407.0 29.706-13.299 29.706-29.701.0-16.403-13.299-29.702-29.706-29.702-11.666.0-21.657 6.792-26.515 16.578l-69.105-14.69c-1.922-.418-3.939-.042-5.585 1.036-1.658 1.073-2.811 2.761-3.224 4.686l-21.152 99.438c-44.258 1.228-84.046 14.494-112.837 35.232-7.468-7.164-17.589-11.591-28.757-11.591-22.965.0-41.585 18.616-41.585 41.58.0 16.896 10.095 31.41 24.568 37.918-.639 4.135-.99 8.328-.99 12.576.0 63.977 74.469 115.836 166.33 115.836s166.334-51.859 166.334-115.836c0-4.218-.347-8.387-.977-12.493 14.564-6.47 24.735-21.034 24.735-38.001zM326.526 373.831c-20.27 20.241-59.115 21.816-70.534 21.816-11.428.0-50.277-1.575-70.522-21.82-3.007-3.008-3.007-7.882.0-10.889 3.003-2.999 7.882-3.003 10.885.0 12.777 12.781 40.11 17.317 59.637 17.317 19.522.0 46.86-4.536 59.657-17.321 3.016-2.999 7.886-2.995 10.885.008 3.008 3.011 3.003 7.882-.008 10.889zm-5.23-48.781c-16.373.0-29.701-13.324-29.701-29.698.0-16.381 13.328-29.714 29.701-29.714 16.378.0 29.706 13.333 29.706 29.714.0 16.374-13.328 29.698-29.706 29.698zM160.91 295.348c0-16.381 13.328-29.71 29.714-29.71 16.369.0 29.689 13.329 29.689 29.71.0 16.373-13.32 29.693-29.689 29.693-16.386.0-29.714-13.32-29.714-29.693z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Apache Airflow Quick start: Installation, Demo DAG, Variables, XComs & Advanced @dag Settings on facebook" href="https://facebook.com/sharer/sharer.php?u=http%3a%2f%2flocalhost%3a52095%2fShareNotes%2fposts%2fdata-pipeline%2fairflow%2fbasic-airflow%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H342.978V319.085h66.6l12.672-82.621h-79.272v-53.617c0-22.603 11.073-44.636 46.58-44.636H425.6v-70.34s-32.71-5.582-63.982-5.582c-65.288.0-107.96 39.569-107.96 111.204v62.971h-72.573v82.621h72.573V512h-191.104c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Apache Airflow Quick start: Installation, Demo DAG, Variables, XComs & Advanced @dag Settings on whatsapp" href="https://api.whatsapp.com/send?text=Apache%e2%80%afAirflow%20Quick%20start%3a%20Installation%2c%20Demo%20DAG%2c%20Variables%2c%20XComs%20%26%20Advanced%e2%80%af%40dag%20Settings%20-%20http%3a%2f%2flocalhost%3a52095%2fShareNotes%2fposts%2fdata-pipeline%2fairflow%2fbasic-airflow%2f"><svg viewBox="0 0 512 512" height="30" width="30" fill="currentcolor"><path d="M449.446.0C483.971.0 512 28.03 512 62.554v386.892C512 483.97 483.97 512 449.446 512H62.554c-34.524.0-62.554-28.03-62.554-62.554V62.554c0-34.524 28.029-62.554 62.554-62.554h386.892zm-58.673 127.703c-33.842-33.881-78.847-52.548-126.798-52.568-98.799.0-179.21 80.405-179.249 179.234-.013 31.593 8.241 62.428 23.927 89.612l-25.429 92.884 95.021-24.925c26.181 14.28 55.659 21.807 85.658 21.816h.074c98.789.0 179.206-80.413 179.247-179.243.018-47.895-18.61-92.93-52.451-126.81zM263.976 403.485h-.06c-26.734-.01-52.954-7.193-75.828-20.767l-5.441-3.229-56.386 14.792 15.05-54.977-3.542-5.637c-14.913-23.72-22.791-51.136-22.779-79.287.033-82.142 66.867-148.971 149.046-148.971 39.793.014 77.199 15.531 105.329 43.692 28.128 28.16 43.609 65.592 43.594 105.4-.034 82.149-66.866 148.983-148.983 148.984zm81.721-111.581c-4.479-2.242-26.499-13.075-30.604-14.571-4.105-1.495-7.091-2.241-10.077 2.241-2.986 4.483-11.569 14.572-14.182 17.562-2.612 2.988-5.225 3.364-9.703 1.12-4.479-2.241-18.91-6.97-36.017-22.23C231.8 264.15 222.81 249.484 220.198 245s-.279-6.908 1.963-9.14c2.016-2.007 4.48-5.232 6.719-7.847 2.24-2.615 2.986-4.484 4.479-7.472 1.493-2.99.747-5.604-.374-7.846-1.119-2.241-10.077-24.288-13.809-33.256-3.635-8.733-7.327-7.55-10.077-7.688-2.609-.13-5.598-.158-8.583-.158-2.986.0-7.839 1.121-11.944 5.604-4.105 4.484-15.675 15.32-15.675 37.364.0 22.046 16.048 43.342 18.287 46.332 2.24 2.99 31.582 48.227 76.511 67.627 10.685 4.615 19.028 7.371 25.533 9.434 10.728 3.41 20.492 2.929 28.209 1.775 8.605-1.285 26.499-10.833 30.231-21.295 3.732-10.464 3.732-19.431 2.612-21.298-1.119-1.869-4.105-2.99-8.583-5.232z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Apache Airflow Quick start: Installation, Demo DAG, Variables, XComs & Advanced @dag Settings on telegram" href="https://telegram.me/share/url?text=Apache%e2%80%afAirflow%20Quick%20start%3a%20Installation%2c%20Demo%20DAG%2c%20Variables%2c%20XComs%20%26%20Advanced%e2%80%af%40dag%20Settings&amp;url=http%3a%2f%2flocalhost%3a52095%2fShareNotes%2fposts%2fdata-pipeline%2fairflow%2fbasic-airflow%2f"><svg viewBox="2 2 28 28" height="30" width="30" fill="currentcolor"><path d="M26.49 29.86H5.5a3.37 3.37.0 01-2.47-1 3.35 3.35.0 01-1-2.47V5.48A3.36 3.36.0 013 3 3.37 3.37.0 015.5 2h21A3.38 3.38.0 0129 3a3.36 3.36.0 011 2.46V26.37a3.35 3.35.0 01-1 2.47 3.38 3.38.0 01-2.51 1.02zm-5.38-6.71a.79.79.0 00.85-.66L24.73 9.24a.55.55.0 00-.18-.46.62.62.0 00-.41-.17q-.08.0-16.53 6.11a.59.59.0 00-.41.59.57.57.0 00.43.52l4 1.24 1.61 4.83a.62.62.0 00.63.43.56.56.0 00.4-.17L16.54 20l4.09 3A.9.9.0 0021.11 23.15zM13.8 20.71l-1.21-4q8.72-5.55 8.78-5.55c.15.0.23.0.23.16a.18.18.0 010 .06s-2.51 2.3-7.52 6.8z"/></svg></a></li><li><a target=_blank rel="noopener noreferrer" aria-label="share Apache Airflow Quick start: Installation, Demo DAG, Variables, XComs & Advanced @dag Settings on ycombinator" href="https://news.ycombinator.com/submitlink?t=Apache%e2%80%afAirflow%20Quick%20start%3a%20Installation%2c%20Demo%20DAG%2c%20Variables%2c%20XComs%20%26%20Advanced%e2%80%af%40dag%20Settings&u=http%3a%2f%2flocalhost%3a52095%2fShareNotes%2fposts%2fdata-pipeline%2fairflow%2fbasic-airflow%2f"><svg width="30" height="30" viewBox="0 0 512 512" fill="currentcolor" xmlns:inkscape="http://www.inkscape.org/namespaces/inkscape"><path d="M449.446.0C483.971.0 512 28.03 512 62.554V449.446C512 483.97 483.97 512 449.446 512H62.554C28.03 512 0 483.97.0 449.446V62.554C0 28.03 28.029.0 62.554.0H449.446zM183.8767 87.9921h-62.034L230.6673 292.4508V424.0079h50.6655V292.4508L390.1575 87.9921H328.1233L256 238.2489z"/></svg></a></li></ul></footer></article></main><footer class=footer><span>&copy; 2025 <a href=http://localhost:52095/ShareNotes/>ML & DATA & CODE & CURIOSITY</a></span> ·
<span>Powered by
<a href=https://gohugo.io/ rel="noopener noreferrer" target=_blank>Hugo</a> &
        <a href=https://github.com/adityatelange/hugo-PaperMod/ rel=noopener target=_blank>PaperMod</a></span></footer><a href=#top aria-label="go to top" title="Go to Top (Alt + G)" class=top-link id=top-link accesskey=g><svg viewBox="0 0 12 6" fill="currentcolor"><path d="M12 6H0l6-6z"/></svg>
</a><script>let menu=document.getElementById("menu");menu&&(menu.scrollLeft=localStorage.getItem("menu-scroll-position"),menu.onscroll=function(){localStorage.setItem("menu-scroll-position",menu.scrollLeft)}),document.querySelectorAll('a[href^="#"]').forEach(e=>{e.addEventListener("click",function(e){e.preventDefault();var t=this.getAttribute("href").substr(1);window.matchMedia("(prefers-reduced-motion: reduce)").matches?document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView():document.querySelector(`[id='${decodeURIComponent(t)}']`).scrollIntoView({behavior:"smooth"}),t==="top"?history.replaceState(null,null," "):history.pushState(null,null,`#${t}`)})})</script><script>var mybutton=document.getElementById("top-link");window.onscroll=function(){document.body.scrollTop>800||document.documentElement.scrollTop>800?(mybutton.style.visibility="visible",mybutton.style.opacity="1"):(mybutton.style.visibility="hidden",mybutton.style.opacity="0")}</script><script>document.getElementById("theme-toggle").addEventListener("click",()=>{document.body.className.includes("dark")?(document.body.classList.remove("dark"),localStorage.setItem("pref-theme","light")):(document.body.classList.add("dark"),localStorage.setItem("pref-theme","dark"))})</script></body></html>